{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a86cebfb-ea96-46c2-8d22-a2aa78405033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "488b2e0c-3ee1-44b9-a272-1c103344cf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 12:19:09.792550: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745306349.808983    6417 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745306349.814074    6417 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745306349.827436    6417 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745306349.827450    6417 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745306349.827452    6417 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745306349.827454    6417 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-22 12:19:09.831843: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "wer_metric = load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d09a3e1-208e-4d72-baae-02d90707b932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dffdf6ea-d960-4b5c-ad2b-cc3455debe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('annotations/captions_train2014.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "178db287-e4a7-478a-ba2a-8f0f1be350dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414113"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbefe774-f02b-4eda-8b7f-d2e91a0ceabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2filename = {i['id']:i['file_name'] for i in data['images']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a535ca91-10d4-42bc-af88-9e51b7b97b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f6209f4-cc4a-4f74-b5a7-413113fdf98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['file_name'] = df['image_id'].map(id2filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00010f23-2d47-490c-a5b8-a6a715455ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>318556</td>\n",
       "      <td>48</td>\n",
       "      <td>A very clean and well decorated empty bathroom</td>\n",
       "      <td>COCO_train2014_000000318556.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116100</td>\n",
       "      <td>67</td>\n",
       "      <td>A panoramic view of a kitchen and all of its a...</td>\n",
       "      <td>COCO_train2014_000000116100.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>318556</td>\n",
       "      <td>126</td>\n",
       "      <td>A blue and white bathroom with butterfly theme...</td>\n",
       "      <td>COCO_train2014_000000318556.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>116100</td>\n",
       "      <td>148</td>\n",
       "      <td>A panoramic photo of a kitchen and dining room</td>\n",
       "      <td>COCO_train2014_000000116100.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>379340</td>\n",
       "      <td>173</td>\n",
       "      <td>A graffiti-ed stop sign across the street from...</td>\n",
       "      <td>COCO_train2014_000000379340.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414108</th>\n",
       "      <td>133071</td>\n",
       "      <td>829655</td>\n",
       "      <td>a slice of bread is covered with a sour cream ...</td>\n",
       "      <td>COCO_train2014_000000133071.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414109</th>\n",
       "      <td>410182</td>\n",
       "      <td>829658</td>\n",
       "      <td>A long plate hold some fries with some sliders...</td>\n",
       "      <td>COCO_train2014_000000410182.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414110</th>\n",
       "      <td>180285</td>\n",
       "      <td>829665</td>\n",
       "      <td>Two women sit and pose with stuffed animals.</td>\n",
       "      <td>COCO_train2014_000000180285.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414111</th>\n",
       "      <td>133071</td>\n",
       "      <td>829693</td>\n",
       "      <td>White Plate with a lot of guacamole and an ext...</td>\n",
       "      <td>COCO_train2014_000000133071.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414112</th>\n",
       "      <td>133071</td>\n",
       "      <td>829717</td>\n",
       "      <td>A dinner plate has a lemon wedge garnishment.</td>\n",
       "      <td>COCO_train2014_000000133071.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414113 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        image_id      id                                            caption  \\\n",
       "0         318556      48     A very clean and well decorated empty bathroom   \n",
       "1         116100      67  A panoramic view of a kitchen and all of its a...   \n",
       "2         318556     126  A blue and white bathroom with butterfly theme...   \n",
       "3         116100     148     A panoramic photo of a kitchen and dining room   \n",
       "4         379340     173  A graffiti-ed stop sign across the street from...   \n",
       "...          ...     ...                                                ...   \n",
       "414108    133071  829655  a slice of bread is covered with a sour cream ...   \n",
       "414109    410182  829658  A long plate hold some fries with some sliders...   \n",
       "414110    180285  829665       Two women sit and pose with stuffed animals.   \n",
       "414111    133071  829693  White Plate with a lot of guacamole and an ext...   \n",
       "414112    133071  829717      A dinner plate has a lemon wedge garnishment.   \n",
       "\n",
       "                              file_name  \n",
       "0       COCO_train2014_000000318556.jpg  \n",
       "1       COCO_train2014_000000116100.jpg  \n",
       "2       COCO_train2014_000000318556.jpg  \n",
       "3       COCO_train2014_000000116100.jpg  \n",
       "4       COCO_train2014_000000379340.jpg  \n",
       "...                                 ...  \n",
       "414108  COCO_train2014_000000133071.jpg  \n",
       "414109  COCO_train2014_000000410182.jpg  \n",
       "414110  COCO_train2014_000000180285.jpg  \n",
       "414111  COCO_train2014_000000133071.jpg  \n",
       "414112  COCO_train2014_000000133071.jpg  \n",
       "\n",
       "[414113 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81c81d69-a021-4563-ac3b-771465d603da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44535\n"
     ]
    }
   ],
   "source": [
    "words = sorted(list(set(' '.join(df['caption']).split())))\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b761f0f-0a74-4f64-b2de-66e1d50cb3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44536\n"
     ]
    }
   ],
   "source": [
    "vocabulary = [\"[PAD]\"] + words\n",
    "print(len(vocabulary))\n",
    "idx2word = {k:v for k,v in enumerate(vocabulary, start=0)}\n",
    "word2idx = {v:k for k,v in idx2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf93ac46-8415-4d1f-b9d1-deca6521a491",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.1)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e41cf255-5924-4952-9b4a-3186aa18ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = self.df['caption'][idx]\n",
    "        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        return image, text\n",
    "    \n",
    "    def transform(self, image):\n",
    "        \n",
    "        transform_ops = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        return transform_ops(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62e5e3e3-e553-45ce-b371-930a6f10a531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5824 648\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ImageCaptioningDataset(root_dir='train2014/',\n",
    "                           df=train_df)\n",
    "eval_dataset = ImageCaptioningDataset(root_dir='train2014/',\n",
    "                           df=test_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=10, shuffle=True)\n",
    "test_loader = DataLoader(eval_dataset, batch_size=batch_size, num_workers=10, shuffle=False)\n",
    "print(len(train_loader), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0debf98-90d8-4e2a-9bb0-fac2bea612c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44536\n"
     ]
    }
   ],
   "source": [
    "num_words = len(word2idx)\n",
    "print(num_words)\n",
    "rnn_hidden_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5f815d3-f2b6-491e-b9e6-9202afd947a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f055fb1f-4e7a-413a-839c-f838d1bc538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_words, rnn_hidden_size=256, dropout=0.1):\n",
    "        \n",
    "        super(CRNN, self).__init__()\n",
    "        self.num_cwords = num_words\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        resnet_modules = list(resnet.children())[:-3]\n",
    "        self.cnn = nn.Sequential(\n",
    "            *resnet_modules\n",
    "        )\n",
    "        \n",
    "        self.linear1 = nn.Linear(14336, rnn_hidden_size, bias=False)\n",
    "        \n",
    "        self.rnn1 = nn.GRU(input_size=rnn_hidden_size, \n",
    "                            hidden_size=rnn_hidden_size,\n",
    "                            bidirectional=True, \n",
    "                            batch_first=True)\n",
    "        self.rnn2 = nn.GRU(input_size=rnn_hidden_size*2, \n",
    "                            hidden_size=rnn_hidden_size*2,\n",
    "                            bidirectional=True, \n",
    "                            batch_first=True)\n",
    "        self.linear2 = nn.Linear(self.rnn_hidden_size*4, num_words)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        batch = self.cnn(batch)\n",
    "        \n",
    "        batch = batch.permute(0, 3, 1, 2) # [batch_size, width, channels, height]\n",
    "         \n",
    "        batch_size = batch.size(0)\n",
    "        width = batch.size(1)\n",
    "        batch = batch.view(batch_size, width, -1) # [batch_size, T==width, num_features==channels*height]\n",
    "        \n",
    "        batch = self.linear1(batch)\n",
    "        \n",
    "        batch, hidden = self.rnn1(batch)\n",
    "        \n",
    "        batch, hidden = self.rnn2(batch)\n",
    "        \n",
    "        batch = self.linear2(batch)\n",
    "        \n",
    "        batch = batch.permute(1, 0, 2)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "198d4e03-6977-4395-9871-62de67bbb403",
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn = CRNN(num_words, rnn_hidden_size=rnn_hidden_size)\n",
    "crnn = crnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1360c4d6-9d38-41e2-95eb-1f0b87980e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text):\n",
    "    \n",
    "    text_batch_targets = [word2idx[c] for c in text.split()][:14]\n",
    "    text_batch_targets = text_batch_targets + [0] * (14 - len(text_batch_targets))\n",
    "    text_batch_targets = torch.LongTensor(text_batch_targets)\n",
    "    \n",
    "    return text_batch_targets.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f43cfa16-65ff-4de6-92f4-61b532317d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(text_batch_logits):\n",
    "\n",
    "    text_batch_tokens = text_batch_logits.argmax(2) # [T, batch_size]\n",
    "    text_batch_tokens = text_batch_tokens.numpy().T # [batch_size, T]\n",
    "\n",
    "    text_batch_tokens_new = []\n",
    "    for text_tokens in text_batch_tokens:\n",
    "        text = [idx2word[idx] for idx in text_tokens if idx != 0]\n",
    "        text = \" \".join(text)\n",
    "        text_batch_tokens_new.append(text)\n",
    "\n",
    "    return text_batch_tokens_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5b79863-92d7-44d2-951a-05112ea2ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "lr = 1e-3\n",
    "clip_norm = 5\n",
    "\n",
    "criterion = nn.CTCLoss(blank=0)\n",
    "optimizer = optim.AdamW(crnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86b9178f-9637-44ee-bf32-48013ecf8840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(text_batch, text_batch_logits):\n",
    "    \"\"\"\n",
    "    text_batch: list of strings of length equal to batch size\n",
    "    text_batch_logits: Tensor of size([T, batch_size, num_classes])\n",
    "    \"\"\"\n",
    "\n",
    "    text_batch_targets = torch.cat([encode_text(text) for text in text_batch]).to(device)\n",
    "    target_lengths = [int((i > 0).sum()) for i in text_batch_targets]\n",
    "    \n",
    "    loss = criterion(\n",
    "        nn.functional.log_softmax(text_batch_logits, dim=2), \n",
    "        text_batch_targets, \n",
    "        input_lengths=[14]*len(target_lengths), \n",
    "        target_lengths=target_lengths\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa7510e4-63d6-495e-a2ce-7ff41894d467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5687075a13fd4ab4a6b088752d5f28a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:1    Loss:5.1446688430858085   WER:0.8169326445937329\n",
      "\n",
      "Men dressed as bears are riding a motorcycle -> A motorcycle of a street.\n",
      "Two baby giraffes are standing in a grassy field.  -> A grass in a field.\n",
      "Two women are sitting on a bench talking -> A man of standing on a horse.\n",
      "Supplies and tools are arranged on the floor with a hard hat and bag. -> A sitting on a street.\n",
      "some elephants in some tall green grass and some trees -> A in a field.\n",
      "A young blonde girl holding up 2 cell phones. -> A man is front of a phone.\n",
      "A bicycle chained to two poles mounted to the ground  -> A man motorcycle on a street.\n",
      "Man in red and black wet suit riding surfboard. -> A man is surfboard on a surfboard.\n",
      "A big pair of scissors sticking in something by a paper. -> A next on a it.\n",
      "a single crane in mid-flight along a tree scape -> A grass in a field.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:2    Loss:4.696300023512225   WER:0.804455399798553\n",
      "\n",
      "Men dressed as bears are riding a motorcycle -> A man is motorcycle on a motorcycle.\n",
      "Two baby giraffes are standing in a grassy field.  -> A giraffe in grass in a field.\n",
      "Two women are sitting on a bench talking -> A man of standing on a other.\n",
      "Supplies and tools are arranged on the floor with a hard hat and bag. -> A top on a it.\n",
      "some elephants in some tall green grass and some trees -> A herd of field in a field.\n",
      "A young blonde girl holding up 2 cell phones. -> A man is front in a refrigerator.\n",
      "A bicycle chained to two poles mounted to the ground  -> A motorcycle parked on a road.\n",
      "Man in red and black wet suit riding surfboard. -> A man in surfboard on the surfboard.\n",
      "A big pair of scissors sticking in something by a paper. -> A close next of a it.\n",
      "a single crane in mid-flight along a tree scape -> A bird grass in a field.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:3    Loss:4.511418543368255   WER:0.797899128602715\n",
      "\n",
      "Men dressed as bears are riding a motorcycle -> A man motorcycle on a motorcycle.\n",
      "Two baby giraffes are standing in a grassy field.  -> A giraffes in in a field.\n",
      "Two women are sitting on a bench talking -> A man and next on a street.\n",
      "Supplies and tools are arranged on the floor with a hard hat and bag. -> A contents items of a table.\n",
      "some elephants in some tall green grass and some trees -> A of elephants in field.\n",
      "A young blonde girl holding up 2 cell phones. -> A man in room in a room.\n",
      "A bicycle chained to two poles mounted to the ground  -> A bicycle parked on a hydrant.\n",
      "Man in red and black wet suit riding surfboard. -> A man riding a surfboard on the water.\n",
      "A big pair of scissors sticking in something by a paper. -> A on next on a table.\n",
      "a single crane in mid-flight along a tree scape -> A bird grass on a grass.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:4    Loss:4.366688799645777   WER:0.7955381317168282\n",
      "\n",
      "Men dressed as bears are riding a motorcycle -> A next on luggage.\n",
      "Two baby giraffes are standing in a grassy field.  -> Two giraffes in in a field.\n",
      "Two women are sitting on a bench talking -> A man next in a phone.\n",
      "Supplies and tools are arranged on the floor with a hard hat and bag. -> A next on table.\n",
      "some elephants in some tall green grass and some trees -> A herd of elephants in field.\n",
      "A young blonde girl holding up 2 cell phones. -> A woman girl front in a computer.\n",
      "A bicycle chained to two poles mounted to the ground  -> A next on a meter.\n",
      "Man in red and black wet suit riding surfboard. -> A man is surfboard in the surfing.\n",
      "A big pair of scissors sticking in something by a paper. -> A pair scissors on a surface.\n",
      "a single crane in mid-flight along a tree scape -> A giraffe in grass in a grass.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:5    Loss:4.202155543639228   WER:0.7870274540968609\n",
      "\n",
      "Men dressed as bears are riding a motorcycle -> A man motorcycle on a motorcycle.\n",
      "Two baby giraffes are standing in a grassy field.  -> A giraffe in in a field.\n",
      "Two women are sitting on a bench talking -> A man and talking on a street\n",
      "Supplies and tools are arranged on the floor with a hard hat and bag. -> A next on a it.\n",
      "some elephants in some tall green grass and some trees -> A of elephants in a field.\n",
      "A young blonde girl holding up 2 cell phones. -> A man in glass in a mirror.\n",
      "A bicycle chained to two poles mounted to the ground  -> A red next on a meter.\n",
      "Man in red and black wet suit riding surfboard. -> A man man a surfboard in a water.\n",
      "A big pair of scissors sticking in something by a paper. -> A pair on top on a table.\n",
      "a single crane in mid-flight along a tree scape -> A bird bird a bird on a branch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:6    Loss:4.089978411819732   WER:0.7947110897548444\n",
      "\n",
      "Men dressed as bears are riding a motorcycle -> A woman motorcycle of a motorcycle.\n",
      "Two baby giraffes are standing in a grassy field.  -> Two giraffes in a field.\n",
      "Two women are sitting on a bench talking -> A man and talking on a together.\n",
      "Supplies and tools are arranged on the floor with a hard hat and bag. -> A items next on floor.\n",
      "some elephants in some tall green grass and some trees -> A herd of elephants in grass.\n",
      "A young blonde girl holding up 2 cell phones. -> A woman in front in a phone.\n",
      "A bicycle chained to two poles mounted to the ground  -> A bicycle bicycle next of a it.\n",
      "Man in red and black wet suit riding surfboard. -> A woman woman surfboard in a ocean.\n",
      "A big pair of scissors sticking in something by a paper. -> A pair scissors on top on a table.\n",
      "a single crane in mid-flight along a tree scape -> A bird bird flight in a tree.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:7    Loss:4.000036643678143   WER:0.7850014323073085\n",
      "\n",
      "Men dressed as bears are riding a motorcycle -> A man of a motorcycle on a motorcycle.\n",
      "Two baby giraffes are standing in a grassy field.  -> A giraffe giraffe in a in a field.\n",
      "Two women are sitting on a bench talking -> A man and talking on a street.\n",
      "Supplies and tools are arranged on the floor with a hard hat and bag. -> A next on a assignment\n",
      "some elephants in some tall green grass and some trees -> A herd of in field.\n",
      "A young blonde girl holding up 2 cell phones. -> A woman is talking on her phone.\n",
      "A bicycle chained to two poles mounted to the ground  -> A sitting on a meter.\n",
      "Man in red and black wet suit riding surfboard. -> A man on a surfboard in a water\n",
      "A big pair of scissors sticking in something by a paper. -> A pair next on table.\n",
      "a single crane in mid-flight along a tree scape -> A bird bird flight in the field.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:8    Loss:3.9242680596172748   WER:0.7891320217710711\n",
      "\n",
      "Men dressed as bears are riding a motorcycle -> A in taxes\n",
      "Two baby giraffes are standing in a grassy field.  -> Two giraffes in in a field.\n",
      "Two women are sitting on a bench talking -> A man of on a street.\n",
      "Supplies and tools are arranged on the floor with a hard hat and bag. -> A hardhat electronics on a assignment\n",
      "some elephants in some tall green grass and some trees -> A of elephants in grass.\n",
      "A young blonde girl holding up 2 cell phones. -> A woman is kitchen in a kitchen\n",
      "A bicycle chained to two poles mounted to the ground  -> A chained locked locked on a benches.\n",
      "Man in red and black wet suit riding surfboard. -> A woman in surfboard on the wave.\n",
      "A big pair of scissors sticking in something by a paper. -> A pair scissors on sitting on a paper.\n",
      "a single crane in mid-flight along a tree scape -> A bird bird in day.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b32b37038914d89a26fbfa0148320d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:13    Loss:3.708645371386441   WER:0.787096759289206\n",
      "\n",
      "Men dressed as bears are riding a motorcycle -> A costumes with on a taxes\n",
      "Two baby giraffes are standing in a grassy field.  -> A giraffe on a a field.\n",
      "Two women are sitting on a bench talking -> A man talking on a phone.\n",
      "Supplies and tools are arranged on the floor with a hard hat and bag. -> A items items with neatly on assignment\n",
      "some elephants in some tall green grass and some trees -> A tide of elephants in in field.\n",
      "A young blonde girl holding up 2 cell phones. -> A woman holding front in a computer.\n",
      "A bicycle chained to two poles mounted to the ground  -> A bicycle chained tied on a museum.\n",
      "Man in red and black wet suit riding surfboard. -> A man on on on a wave on the wave.\n",
      "A big pair of scissors sticking in something by a paper. -> A hate on life!\"\n",
      "a single crane in mid-flight along a tree scape -> A giraffe flight flapping the day.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:14    Loss:3.68446057299988   WER:0.790665976694974\n",
      "\n",
      "Men dressed as bears are riding a motorcycle -> A on motorcycle on a motorcycles.\n",
      "Two baby giraffes are standing in a grassy field.  -> Two giraffes giraffe in in field.\n",
      "Two women are sitting on a bench talking -> A woman and skirt, talking on a phone.\n",
      "Supplies and tools are arranged on the floor with a hard hat and bag. -> A hardhat items on items on a it.\n",
      "some elephants in some tall green grass and some trees -> A in a field.\n",
      "A young blonde girl holding up 2 cell phones. -> A woman in broken in a phone.\n",
      "A bicycle chained to two poles mounted to the ground  -> A bicycle chained chained chained sitting in a building\n",
      "Man in red and black wet suit riding surfboard. -> A man man on a wave in the ocean.\n",
      "A big pair of scissors sticking in something by a paper. -> A pair on a next on a table.\n",
      "a single crane in mid-flight along a tree scape -> A bird bird flight flight in day.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:15    Loss:3.659258283229712   WER:0.7816655423824352\n",
      "\n",
      "Men dressed as bears are riding a motorcycle -> A on on a bikes.\n",
      "Two baby giraffes are standing in a grassy field.  -> Two giraffes in in a field.\n",
      "Two women are sitting on a bench talking -> A man and skirt, on a street.\n",
      "Supplies and tools are arranged on the floor with a hard hat and bag. -> A devices items and out on assignment\n",
      "some elephants in some tall green grass and some trees -> A herd of elephants elephants field.\n",
      "A young blonde girl holding up 2 cell phones. -> A woman woman front in a keyboard.\n",
      "A bicycle chained to two poles mounted to the ground  -> A bicycle bicycle next on a benches.\n",
      "Man in red and black wet suit riding surfboard. -> A man on on a surfboard on a wave\n",
      "A big pair of scissors sticking in something by a paper. -> A pair on next on a table.\n",
      "a single crane in mid-flight along a tree scape -> A bird Pelican flight in a day.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:16    Loss:3.6449137307778963   WER:0.7905389171756748\n",
      "\n",
      "Men dressed as bears are riding a motorcycle -> A of bear-costumed bear-costumed motorcycles \"no tax\"\n",
      "Two baby giraffes are standing in a grassy field.  -> Two giraffes standing in in a field.\n",
      "Two women are sitting on a bench talking -> A couple and talking on a conversation.\n",
      "Supplies and tools are arranged on the floor with a hard hat and bag. -> A devices devices on neatly on assignment\n",
      "some elephants in some tall green grass and some trees -> A of elephants walking in the grass.\n",
      "A young blonde girl holding up 2 cell phones. -> A woman holding cellphone in keyboard.\n",
      "A bicycle chained to two poles mounted to the ground  -> A display of a building.\n",
      "Man in red and black wet suit riding surfboard. -> A boy boy board on the surfing.\n",
      "A big pair of scissors sticking in something by a paper. -> A pair of next of a paper.\n",
      "a single crane in mid-flight along a tree scape -> A Pelican Pelican flight in the cloudy day.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a990bcdde51c4dd1ac2c7bc4952d86d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:20    Loss:3.5778672087458925   WER:0.7851977970189526\n",
      "\n",
      "Men dressed as bears are riding a motorcycle -> A people of motorcycle on a taxes\n",
      "Two baby giraffes are standing in a grassy field.  -> Two giraffes in field.\n",
      "Two women are sitting on a bench talking -> A man and on talking on a cell phone.\n",
      "Supplies and tools are arranged on the floor with a hard hat and bag. -> A devices items items on top on a assignment\n",
      "some elephants in some tall green grass and some trees -> A tide elephants walking in in a grass.\n",
      "A young blonde girl holding up 2 cell phones. -> A woman is phones in a phone.\n",
      "A bicycle chained to two poles mounted to the ground  -> A locked sitting on a toothbrushes.\n",
      "Man in red and black wet suit riding surfboard. -> A man is surfboard on a surfing.\n",
      "A big pair of scissors sticking in something by a paper. -> A of a next on a scissors.\n",
      "a single crane in mid-flight along a tree scape -> A bird bird flight in the day.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b7574f542241b4822b575b8fed82b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.amp.GradScaler('cuda', enabled = True)\n",
    "\n",
    "epoch_losses = []\n",
    "iteration_losses = []\n",
    "num_updates_epochs = []\n",
    "for epoch in tqdm(range(1, num_epochs+1)):\n",
    "    \n",
    "    crnn.train()\n",
    "    \n",
    "    epoch_loss_list = [] \n",
    "    num_updates_epoch = 0\n",
    "    for image_batch, text_batch in tqdm(train_loader, leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.amp.autocast('cuda', enabled = True):\n",
    "            text_batch_logits = crnn(image_batch.to(device))\n",
    "            loss = compute_loss(text_batch, text_batch_logits)\n",
    "            \n",
    "        iteration_loss = loss.item()\n",
    "        \n",
    "        if iteration_loss == float('inf'):\n",
    "            continue\n",
    "          \n",
    "        epoch_loss_list.append(iteration_loss)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(crnn.parameters(), clip_norm)\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        scaler.update()\n",
    "\n",
    "    crnn.eval()\n",
    "    \n",
    "    pred_str = []\n",
    "    label_str = []\n",
    "    for image_batch, text_batch in tqdm(test_loader, leave=False):\n",
    "        with torch.amp.autocast('cuda', enabled = True):\n",
    "            text_batch_logits = crnn(image_batch.to(device))\n",
    "            \n",
    "        pred_text_batch = decode_predictions(text_batch_logits.cpu())\n",
    "        \n",
    "        pred_str += pred_text_batch\n",
    "        label_str += text_batch\n",
    "        \n",
    "    epoch_loss = np.mean(epoch_loss_list)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    print()\n",
    "    print(f\"Epoch:{epoch}    Loss:{epoch_loss}   WER:{wer}\")\n",
    "    print()\n",
    "    for p, l in zip(pred_str[:10], label_str[:10]):\n",
    "        print(l, '->', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ef3a9-62ef-4f08-881a-76c7b8bff9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d40d280-aef3-44c0-a131-a2cd76ae4ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
