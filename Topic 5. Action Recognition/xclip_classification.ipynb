{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 21:27:18.005969: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-18 21:27:18.483681: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/vladimir/.virtualenvs/ml/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2024-03-18 21:27:18.483729: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/vladimir/.virtualenvs/ml/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2024-03-18 21:27:18.483734: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import av\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoProcessor, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_video_augmentations(video, transform):\n",
    "    targets={'image': video[0]}\n",
    "    for i in range(1, video.shape[0]):\n",
    "        targets[f'image{i}'] = video[i]\n",
    "    transformed = transform(**targets)\n",
    "    transformed = np.concatenate(\n",
    "        [np.expand_dims(transformed['image'], axis=0)] \n",
    "        + [np.expand_dims(transformed[f'image{i}'], axis=0) for i in range(1, video.shape[0])]\n",
    "    )\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_pyav(container, indices):\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    while converted_len >= seg_len and clip_len > 1:\n",
    "        clip_len -= 1\n",
    "        converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = converted_len\n",
    "    start_idx = 0\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "root_dir = 'UCF-101/'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i for i in os.listdir(root_dir) if i[0] != '.']\n",
    "labels2id = {label:i for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bb6ee908c1499995d08fbe3bd05203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = []\n",
    "for label in tqdm(labels):\n",
    "    for video_name in os.listdir(f'{root_dir}/{label}'):\n",
    "        container = av.open(f'{root_dir}/{label}/{video_name}')\n",
    "        if container.streams.video[0].frames > 75:\n",
    "            train.append({\n",
    "                'label': label,\n",
    "                'video_path': f'{root_dir}/{label}/{video_name}'\n",
    "            })\n",
    "train = pd.DataFrame(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PlayingCello         164\n",
       "PlayingDhol          164\n",
       "HorseRiding          163\n",
       "BoxingPunchingBag    162\n",
       "Drumming             161\n",
       "                    ... \n",
       "BodyWeightSquats      90\n",
       "JavelinThrow          82\n",
       "BlowingCandles        68\n",
       "BasketballDunk        57\n",
       "PushUps               54\n",
       "Name: label, Length: 101, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label_id'] = train.label.map(labels2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, _, _ = train_test_split(train, train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.5, contrast_limit=0.5, p=0.5)\n",
    "], additional_targets={\n",
    "    f'image{i}': 'image'\n",
    "    for i in range(1, 8)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionDataset(Dataset):\n",
    "\n",
    "    def __init__(self, meta, transform=None):\n",
    "        self.meta = meta\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                file_path = self.meta['video_path'].iloc[idx]\n",
    "                container = av.open(file_path)\n",
    "                indices = sample_frame_indices(clip_len=8, frame_sample_rate=5, seg_len=container.streams.video[0].frames)\n",
    "                video = read_video_pyav(container, indices)\n",
    "                while video.shape[0] < 8:\n",
    "                    video = np.vstack([video, video[-1:]])\n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "            break\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = apply_video_augmentations(video, self.transform)\n",
    "            \n",
    "\n",
    "        inputs = processor(\n",
    "            text=[''],\n",
    "            videos=list(video),\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        )\n",
    "        for i in inputs:\n",
    "            inputs[i] = inputs[i][0]\n",
    "\n",
    "        return inputs, self.meta['label_id'].iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ActionDataset(meta=X_train, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=15)\n",
    "\n",
    "val_dataset = ActionDataset(meta=X_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=101, bias=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
    "model.to(device)\n",
    "classifier = nn.Linear(512, len(labels))\n",
    "classifier.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen XClip training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "freeze_epochs = 5\n",
    "model_lr = 1e-5\n",
    "classifier_lr = 1e-3\n",
    "\n",
    "model_optimizer = optim.AdamW(model.parameters(), model_lr)\n",
    "classifier_optimizer = torch.optim.AdamW(classifier.parameters(), lr=classifier_lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c402ec03176a4aa78cbf8089d074befb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 0:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.018884596971115\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3247403ce24b6988986a00b463b736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 0:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.433330998128774\n",
      "F1: 0.7946440619920973\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7d505a41984ddbadd7839dd27498ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 1:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.900474670397137\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0243358efb104c93a74b91ec64d99099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 1:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.4261778544406503\n",
      "F1: 0.8477762722676296\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db72eab09ade4417b2eef67162980ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 2:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.006405236777999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba6fedd54f340838e76eb8eea5f87a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 2:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.679323940860982\n",
      "F1: 0.8728186861949377\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574e7b9c6fda404c8f514f3bb077dba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 3:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.3875153754351488\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe6d516c3534e61b2b69b5c662b1313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 3:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.1925898747784751\n",
      "F1: 0.8929003187479272\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500d609656ec41c394881b0ef6af0e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 4:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.9996980937267733\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132b8fb223d744258a8b3e7365122459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 4:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.8944721252334361\n",
      "F1: 0.9112310784445119\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    model.eval()\n",
    "    classifier.train()     \n",
    "\n",
    "    train_loss = []\n",
    "    for i, (batch, targets) in enumerate(tqdm(train_dataloader, desc=f\"Epoch: {epoch}\")):\n",
    "        classifier_optimizer.zero_grad()\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        logits = classifier(outputs.video_embeds)\n",
    "\n",
    "        loss = criterion(logits, targets) \n",
    "        loss.backward()\n",
    "        classifier_optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    print('Training loss:', np.mean(train_loss))\n",
    "\n",
    "    model.eval() \n",
    "    classifier.eval()    \n",
    "\n",
    "    val_loss = []\n",
    "    val_targets = []\n",
    "    val_preds = []\n",
    "    for i, (batch, targets) in enumerate(tqdm(val_dataloader, desc=f\"Epoch: {epoch}\")):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            batch = batch.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            logits = classifier(outputs.video_embeds)\n",
    "\n",
    "            loss = criterion(logits, targets) \n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "            val_targets.extend(targets.cpu().numpy())\n",
    "            val_preds.extend(logits.argmax(axis=1).cpu().numpy())\n",
    "\n",
    "    print('Val loss:', np.mean(val_loss))\n",
    "    print('F1:', f1_score(val_targets, val_preds, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full XClip training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.text_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a406d2950a184b8b8e18a8b8cd712fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 0:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.36282466362157373\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8627d64655453a812fb72a8e0a63db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 0:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.2345287130803478\n",
      "F1: 0.9609115342441644\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21eafc7540184f2b9d0f3e3ac44d9582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 1:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.101868323689333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8642501d725b42d09c8ed5b2e9510471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 1:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.12485515451704969\n",
      "F1: 0.9829743104295883\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3dbc69b06d2486790f51604a302ddac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 2:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.04672228425436052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2f98ac53734a82a100650dcb020407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 2:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.09608748642613693\n",
      "F1: 0.985137991482419\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1da85b1ce94ea3acaf3345cdac4071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 3:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.03176195842592179\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d717b5e356445b6ab1f758dc476903c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 3:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.07797025535635803\n",
      "F1: 0.9858082602909529\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2860c953f9845c4873cf4c762bed453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 4:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.02439089220195195\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55be73e63804423f815a2dd256704137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 4:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.06936652772128582\n",
      "F1: 0.9860634274981412\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train() \n",
    "    classifier.train()     \n",
    "\n",
    "    train_loss = []\n",
    "    for i, (batch, targets) in enumerate(tqdm(train_dataloader, desc=f\"Epoch: {epoch}\")):\n",
    "        model_optimizer.zero_grad()\n",
    "        classifier_optimizer.zero_grad()\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        logits = classifier(outputs.video_embeds)\n",
    "\n",
    "        loss = criterion(logits, targets) \n",
    "        loss.backward()\n",
    "        model_optimizer.step()\n",
    "        classifier_optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    print('Training loss:', np.mean(train_loss))\n",
    "\n",
    "    model.eval()\n",
    "    classifier.eval()   \n",
    "\n",
    "    val_loss = []\n",
    "    val_targets = []\n",
    "    val_preds = []\n",
    "    for i, (batch, targets) in enumerate(tqdm(val_dataloader, desc=f\"Epoch: {epoch}\")):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            batch = batch.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            logits = classifier(outputs.video_embeds)\n",
    "\n",
    "            loss = criterion(logits, targets) \n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "            val_targets.extend(targets.cpu().numpy())\n",
    "            val_preds.extend(logits.argmax(axis=1).cpu().numpy())           \n",
    "\n",
    "    print('Val loss:', np.mean(val_loss))\n",
    "    print('F1:', f1_score(val_targets, val_preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
