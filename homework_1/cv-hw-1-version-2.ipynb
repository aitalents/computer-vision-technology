{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"!pip install torch\n!pip install torchvision\n!pip install numpy\n!pip install pandas\n!pip install thop\n!pip install tqdm\n!pip install fiftyone\n!pip install opencv-python\n!pip install matplotlib\n!pip install pillow","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-27T15:54:51.901284Z","iopub.execute_input":"2024-04-27T15:54:51.902197Z","iopub.status.idle":"2024-04-27T15:57:02.545021Z","shell.execute_reply.started":"2024-04-27T15:54:51.902164Z","shell.execute_reply":"2024-04-27T15:57:02.543959Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.1.2)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchvision) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchvision) (1.3.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.1.4)\nRequirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: thop in /opt/conda/lib/python3.10/site-packages (0.1.1.post2209072238)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from thop) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->thop) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->thop) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->thop) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->thop) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->thop) (1.3.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\nRequirement already satisfied: fiftyone in /opt/conda/lib/python3.10/site-packages (0.23.8)\nRequirement already satisfied: aiofiles in /opt/conda/lib/python3.10/site-packages (from fiftyone) (22.1.0)\nRequirement already satisfied: argcomplete in /opt/conda/lib/python3.10/site-packages (from fiftyone) (3.3.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (4.12.2)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (1.26.100)\nRequirement already satisfied: cachetools in /opt/conda/lib/python3.10/site-packages (from fiftyone) (4.2.4)\nRequirement already satisfied: dacite<1.8.0,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (1.7.0)\nRequirement already satisfied: Deprecated in /opt/conda/lib/python3.10/site-packages (from fiftyone) (1.2.14)\nRequirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from fiftyone) (6.2.0)\nRequirement already satisfied: humanize in /opt/conda/lib/python3.10/site-packages (from fiftyone) (4.9.0)\nRequirement already satisfied: hypercorn>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.16.0)\nRequirement already satisfied: Jinja2>=3 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (3.1.2)\nRequirement already satisfied: kaleido!=0.2.1.post1 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.2.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from fiftyone) (3.7.5)\nRequirement already satisfied: mongoengine==0.24.2 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.24.2)\nRequirement already satisfied: motor>=2.5 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (3.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fiftyone) (1.26.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from fiftyone) (21.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from fiftyone) (2.1.4)\nRequirement already satisfied: Pillow>=6.2 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (9.5.0)\nRequirement already satisfied: plotly>=4.14 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (5.18.0)\nRequirement already satisfied: pprintpp in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.4.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from fiftyone) (5.9.3)\nRequirement already satisfied: pymongo>=3.12 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (4.7.0)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from fiftyone) (2023.3.post1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from fiftyone) (6.0.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from fiftyone) (2023.12.25)\nRequirement already satisfied: retrying in /opt/conda/lib/python3.10/site-packages (from fiftyone) (1.3.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from fiftyone) (1.2.2)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.22.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from fiftyone) (1.11.4)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from fiftyone) (69.0.3)\nRequirement already satisfied: sseclient-py<2,>=1.7.2 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (1.8.0)\nRequirement already satisfied: sse-starlette<1,>=0.10.3 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.10.3)\nRequirement already satisfied: starlette>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.32.0.post1)\nRequirement already satisfied: strawberry-graphql==0.138.1 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.138.1)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.9.0)\nRequirement already satisfied: xmltodict in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.13.0)\nRequirement already satisfied: universal-analytics-python3<2,>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (1.1.1)\nRequirement already satisfied: fiftyone-brain<0.17,>=0.16.1 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.16.1)\nRequirement already satisfied: fiftyone-db<2.0,>=0.4 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (1.1.2)\nRequirement already satisfied: voxel51-eta<0.13,>=0.12.6 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.12.6)\nRequirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (from fiftyone) (4.9.0.80)\nRequirement already satisfied: graphql-core<3.3.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from strawberry-graphql==0.138.1->fiftyone) (3.2.3)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from strawberry-graphql==0.138.1->fiftyone) (2.9.0.post0)\nRequirement already satisfied: typing_extensions<5.0.0,>=3.7.4 in /opt/conda/lib/python3.10/site-packages (from strawberry-graphql==0.138.1->fiftyone) (4.9.0)\nRequirement already satisfied: h11 in /opt/conda/lib/python3.10/site-packages (from hypercorn>=0.13.2->fiftyone) (0.14.0)\nRequirement already satisfied: h2>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from hypercorn>=0.13.2->fiftyone) (4.1.0)\nRequirement already satisfied: priority in /opt/conda/lib/python3.10/site-packages (from hypercorn>=0.13.2->fiftyone) (2.0.0)\nRequirement already satisfied: taskgroup in /opt/conda/lib/python3.10/site-packages (from hypercorn>=0.13.2->fiftyone) (0.0.0a4)\nRequirement already satisfied: tomli in /opt/conda/lib/python3.10/site-packages (from hypercorn>=0.13.2->fiftyone) (2.0.1)\nRequirement already satisfied: wsproto>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from hypercorn>=0.13.2->fiftyone) (1.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3->fiftyone) (2.1.3)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly>=4.14->fiftyone) (8.2.3)\nRequirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from pymongo>=3.12->fiftyone) (2.6.1)\nRequirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette>=0.24.0->fiftyone) (4.2.0)\nRequirement already satisfied: httpx>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from universal-analytics-python3<2,>=1.0.1->fiftyone) (0.27.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from voxel51-eta<0.13,>=0.12.6->fiftyone) (0.3.8)\nRequirement already satisfied: future in /opt/conda/lib/python3.10/site-packages (from voxel51-eta<0.13,>=0.12.6->fiftyone) (1.0.0)\nRequirement already satisfied: glob2 in /opt/conda/lib/python3.10/site-packages (from voxel51-eta<0.13,>=0.12.6->fiftyone) (0.7)\nRequirement already satisfied: jsonlines in /opt/conda/lib/python3.10/site-packages (from voxel51-eta<0.13,>=0.12.6->fiftyone) (4.0.0)\nRequirement already satisfied: py7zr in /opt/conda/lib/python3.10/site-packages (from voxel51-eta<0.13,>=0.12.6->fiftyone) (0.21.0)\nRequirement already satisfied: rarfile in /opt/conda/lib/python3.10/site-packages (from voxel51-eta<0.13,>=0.12.6->fiftyone) (4.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from voxel51-eta<0.13,>=0.12.6->fiftyone) (2.31.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from voxel51-eta<0.13,>=0.12.6->fiftyone) (1.16.0)\nRequirement already satisfied: sortedcontainers in /opt/conda/lib/python3.10/site-packages (from voxel51-eta<0.13,>=0.12.6->fiftyone) (2.4.0)\nRequirement already satisfied: tzlocal in /opt/conda/lib/python3.10/site-packages (from voxel51-eta<0.13,>=0.12.6->fiftyone) (5.2)\nRequirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from voxel51-eta<0.13,>=0.12.6->fiftyone) (1.26.18)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->fiftyone) (2.5)\nRequirement already satisfied: botocore<1.30.0,>=1.29.100 in /opt/conda/lib/python3.10/site-packages (from boto3->fiftyone) (1.29.165)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3->fiftyone) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3->fiftyone) (0.6.2)\nRequirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from Deprecated->fiftyone) (1.14.1)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->fiftyone) (0.2.13)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fiftyone) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fiftyone) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fiftyone) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fiftyone) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fiftyone) (3.1.1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->fiftyone) (2023.4)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->fiftyone) (3.2.1)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image->fiftyone) (2.33.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->fiftyone) (2023.12.9)\nRequirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image->fiftyone) (0.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->fiftyone) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->fiftyone) (3.2.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette>=0.24.0->fiftyone) (3.6)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette>=0.24.0->fiftyone) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette>=0.24.0->fiftyone) (1.2.0)\nRequirement already satisfied: hyperframe<7,>=6.0 in /opt/conda/lib/python3.10/site-packages (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone) (6.0.1)\nRequirement already satisfied: hpack<5,>=4.0 in /opt/conda/lib/python3.10/site-packages (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone) (4.0.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (1.0.5)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines->voxel51-eta<0.13,>=0.12.6->fiftyone) (23.2.0)\nRequirement already satisfied: texttable in /opt/conda/lib/python3.10/site-packages (from py7zr->voxel51-eta<0.13,>=0.12.6->fiftyone) (1.7.0)\nRequirement already satisfied: pycryptodomex>=3.16.0 in /opt/conda/lib/python3.10/site-packages (from py7zr->voxel51-eta<0.13,>=0.12.6->fiftyone) (3.20.0)\nRequirement already satisfied: pyzstd>=0.15.9 in /opt/conda/lib/python3.10/site-packages (from py7zr->voxel51-eta<0.13,>=0.12.6->fiftyone) (0.15.10)\nRequirement already satisfied: pyppmd<1.2.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from py7zr->voxel51-eta<0.13,>=0.12.6->fiftyone) (1.1.0)\nRequirement already satisfied: pybcj<1.1.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from py7zr->voxel51-eta<0.13,>=0.12.6->fiftyone) (1.0.2)\nRequirement already satisfied: multivolumefile>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from py7zr->voxel51-eta<0.13,>=0.12.6->fiftyone) (0.2.3)\nRequirement already satisfied: inflate64<1.1.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from py7zr->voxel51-eta<0.13,>=0.12.6->fiftyone) (1.0.0)\nRequirement already satisfied: brotli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from py7zr->voxel51-eta<0.13,>=0.12.6->fiftyone) (1.1.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->voxel51-eta<0.13,>=0.12.6->fiftyone) (3.3.2)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (4.9.0.80)\nRequirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from opencv-python) (1.26.4)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: numpy<2,>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (9.5.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport sys\nimport time\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torchvision\nimport fiftyone as fo\nimport fiftyone.zoo as foz\nimport fiftyone.utils.coco as fouc\nfrom tqdm import tqdm\nfrom itertools import product","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:57:56.824816Z","iopub.execute_input":"2024-04-27T15:57:56.825280Z","iopub.status.idle":"2024-04-27T15:57:56.832494Z","shell.execute_reply.started":"2024-04-27T15:57:56.825224Z","shell.execute_reply":"2024-04-27T15:57:56.831451Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"code","source":"class FiftyOneTorchDataset(torch.utils.data.Dataset):\n    \"\"\"A class to construct a PyTorch dataset from a FiftyOne dataset.\n\n    Args:\n        fiftyone_dataset: a FiftyOne dataset or view that will be used for training or testing\n        transforms (None): a list of PyTorch transforms to apply to images and targets when loading\n        gt_field (\"ground_truth\"): the name of the field in fiftyone_dataset that contains the\n            desired labels to load\n        classes (None): a list of class strings that are used to define the mapping between\n            class names and indices. If None, it will use all classes present in the given fiftyone_dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        fiftyone_dataset,\n        transforms=None,\n        gt_field=\"ground_truth\",\n        classes=None,\n    ):\n        self.samples = fiftyone_dataset\n        self.transforms = transforms\n        self.gt_field = gt_field\n\n        self.img_paths = self.samples.values(\"filepath\")\n\n        self.classes = classes\n        if not self.classes:\n            # Get list of distinct labels that exist in the view\n            self.classes = self.samples.distinct(\n                \"%s.detections.label\" % gt_field\n            )\n\n        if self.classes[0] != \"background\":\n            self.classes = [\"background\"] + self.classes\n\n        self.labels_map_rev = {c: i for i, c in enumerate(self.classes)}\n\n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n        sample = self.samples[img_path]\n        metadata = sample.metadata\n        img = Image.open(img_path).convert(\"RGB\")\n        width, height = img.size\n\n        boxes = []\n        labels = []\n        area = []\n        iscrowd = []\n        if sample[self.gt_field] is not None:\n            detections = sample[self.gt_field].detections\n            for det in detections:\n                category_id = self.labels_map_rev[det.label]\n                coco_obj = fouc.COCOObject.from_label(\n                    det, metadata, category_id=category_id,\n                )\n                x, y, w, h = coco_obj.bbox\n                boxes.append([(x + w / 2) / width, (y + h / 2) / height, w / width, h / height]) # normalized (xc, yc, w, h)\n                labels.append(coco_obj.category_id)\n                area.append(coco_obj.area)\n                iscrowd.append(coco_obj.iscrowd)\n\n        target = {}\n        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n        target[\"image_id\"] = torch.as_tensor([idx])\n        target[\"area\"] = torch.as_tensor(area, dtype=torch.float32)\n        target[\"iscrowd\"] = torch.as_tensor(iscrowd, dtype=torch.int64)\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def get_classes(self):\n        return self.classes","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:20:34.109836Z","iopub.execute_input":"2024-04-27T15:20:34.110192Z","iopub.status.idle":"2024-04-27T15:20:34.125687Z","shell.execute_reply.started":"2024-04-27T15:20:34.110165Z","shell.execute_reply":"2024-04-27T15:20:34.124771Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Transformtsz:\n    def __init__(self, resize):\n        self.resize = resize\n    def __call__(self, image, boxes):\n        image = torchvision.transforms.functional.resize(image, self.resize)\n        image = torchvision.transforms.functional.to_tensor(image)\n        return image, boxes","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:20:37.176901Z","iopub.execute_input":"2024-04-27T15:20:37.177789Z","iopub.status.idle":"2024-04-27T15:20:37.183004Z","shell.execute_reply.started":"2024-04-27T15:20:37.177756Z","shell.execute_reply":"2024-04-27T15:20:37.182063Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def collate(batch, grid_size=7, n_classes=80):\n    images = []\n    gt = []\n    for item in batch:\n        images.append(item[0].unsqueeze(0))\n\n        fmap = torch.zeros(1, grid_size, grid_size, 5*2+n_classes)\n        bboxes = item[1][\"boxes\"]\n        labels = item[1][\"labels\"]\n\n        used_col_row = {(r, c): 0 for r,c in list(product(range(7), repeat=2))}\n        for bbox, label in zip(bboxes, labels):\n            col = int(bbox[1] * grid_size)\n            row = int(bbox[0] * grid_size)\n            cell_size = 1 / grid_size\n            row_interval = (cell_size*row, cell_size*(row+1))\n            col_interval = (cell_size*col, cell_size*(col+1))\n\n            # if more than 2 bboxes in one cell then skip\n            if used_col_row[(row, col)] == 2:\n                continue\n\n            used_col_row[(row, col)] += 1\n\n            if used_col_row[(row, col)] == 1:\n                # bbox center coords relative to grid cell\n                fmap[0, row, col, 0]  = (bbox[0] - row_interval[0]) / (row_interval[1] - row_interval[0])\n                fmap[0, row, col, 1]  = (bbox[1] - col_interval[0]) / (col_interval[1] - col_interval[0])\n                fmap[0, row, col, 2:4]  = bbox[2:] # bbox w and h relative to image size\n                fmap[0, row, col, 4] = 1 # confindece\n            elif used_col_row[(row, col)] == 2:\n                # bbox center coords relative to grid cell\n                fmap[0, row, col, 5]  = (bbox[0] - row_interval[0]) / (row_interval[1] - row_interval[0])\n                fmap[0, row, col, 6]  = (bbox[1] - col_interval[0]) / (col_interval[1] - col_interval[0])\n                fmap[0, row, col, 7:9]  = bbox[2:] # bbox w and h relative to image size\n                fmap[0, row, col, 9] = 1 # confindece\n            # set classes probabilities\n            fmap[0, row, col, label - 1 + 10] = 1\n        gt.append(fmap)\n    \n    images = torch.cat(images, 0)\n    detections = torch.cat(gt, 0)\n    return (images, detections)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:20:45.974554Z","iopub.execute_input":"2024-04-27T15:20:45.974948Z","iopub.status.idle":"2024-04-27T15:20:45.989491Z","shell.execute_reply.started":"2024-04-27T15:20:45.974918Z","shell.execute_reply":"2024-04-27T15:20:45.988455Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"###### delete\ndef convert_label_matrix_to_bboxes(label_matrix, S=7, C=5, B=2, img_size=448):\n    height, width, _ = label_matrix.shape\n\n    bboxes = []\n    predicted_classes = []\n    confidence = []\n\n    for i in range(height):\n        for j in range(width):\n            cell_label = label_matrix[i, j]\n\n            if cell_label[C] != 0:\n                cell_x, cell_y, width_cell, height_cell = cell_label[C+1:C+5]\n\n                xmin = (j + cell_x - width_cell / 2).item() / S * img_size\n                ymin = (i + cell_y - height_cell / 2).item() / S * img_size\n                xmax = (j + cell_x + width_cell / 2).item() / S * img_size\n                ymax = (i + cell_y + height_cell / 2).item() / S * img_size\n\n                bboxes.append([xmin, ymin, xmax, ymax])\n                predicted_classes.append(torch.argmax(cell_label[0:C]).item())\n                confidence.append(cell_label[C])\n\n    return np.array(bboxes), np.array(predicted_classes), np.array(confidence)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T07:48:21.267032Z","iopub.execute_input":"2024-04-27T07:48:21.267757Z","iopub.status.idle":"2024-04-27T07:48:21.276331Z","shell.execute_reply.started":"2024-04-27T07:48:21.267724Z","shell.execute_reply":"2024-04-27T07:48:21.275390Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# cv_dir = os.getcwd()\n# data_dir = os.path.join(cv_dir, \"data\")\ndata_dir = '/kaggle/working/data'\nfo.config.dataset_zoo_dir = data_dir","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:02:22.714855Z","iopub.execute_input":"2024-04-27T16:02:22.715548Z","iopub.status.idle":"2024-04-27T16:02:22.719998Z","shell.execute_reply.started":"2024-04-27T16:02:22.715515Z","shell.execute_reply":"2024-04-27T16:02:22.719071Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"fo.config.dataset_zoo_dir","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:02:25.558242Z","iopub.execute_input":"2024-04-27T16:02:25.558981Z","iopub.status.idle":"2024-04-27T16:02:25.566833Z","shell.execute_reply.started":"2024-04-27T16:02:25.558952Z","shell.execute_reply":"2024-04-27T16:02:25.565078Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/data'"},"metadata":{}}]},{"cell_type":"code","source":"def load_coco(max_samples, data_dir):\n    dataset = foz.load_zoo_dataset(\n    \"coco-2017\",\n    splits = [\"train\", \"validation\", \"test\"],\n    label_types = [\"detections\"],\n    # classes = classes\n    max_samples = max_samples,\n    dataset_dir=data_dir)\n    \n    dataset.compute_metadata()\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:21:22.239919Z","iopub.execute_input":"2024-04-27T15:21:22.240295Z","iopub.status.idle":"2024-04-27T15:21:22.245620Z","shell.execute_reply.started":"2024-04-27T15:21:22.240266Z","shell.execute_reply":"2024-04-27T15:21:22.244664Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def ttsplit(dataset):\n    train_data = dataset.match_tags(\"train\")\n    test_data = dataset.match_tags(\"test\")\n    val_data = dataset.match_tags(\"validation\")\n    return train_data, test_data, val_data","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:21:24.339355Z","iopub.execute_input":"2024-04-27T15:21:24.340091Z","iopub.status.idle":"2024-04-27T15:21:24.344979Z","shell.execute_reply.started":"2024-04-27T15:21:24.340061Z","shell.execute_reply":"2024-04-27T15:21:24.343964Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def get_torch(dataset):\n    classes = dataset.distinct(\n    \"ground_truth.detections.label\"\n    )\n    torch_dataset = FiftyOneTorchDataset(dataset, transforms=Transformtsz(resize=(448, 448)), classes=classes)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:21:26.273180Z","iopub.execute_input":"2024-04-27T15:21:26.273792Z","iopub.status.idle":"2024-04-27T15:21:26.278869Z","shell.execute_reply.started":"2024-04-27T15:21:26.273760Z","shell.execute_reply":"2024-04-27T15:21:26.277783Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def get_loader(torch_dataset):\n    data_loader = torch.utils.data.DataLoader(torch_dataset, batch_size=1, shuffle=False)\n    return data_loader","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:21:28.059079Z","iopub.execute_input":"2024-04-27T15:21:28.059441Z","iopub.status.idle":"2024-04-27T15:21:28.064533Z","shell.execute_reply.started":"2024-04-27T15:21:28.059414Z","shell.execute_reply":"2024-04-27T15:21:28.063334Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class YoloBackbone(nn.Module):\n    def __init__(self):\n        super(YoloBackbone, self).__init__()\n        conv1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n            # nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.1, inplace=True)\n        )\n        pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        conv2 = nn.Sequential(\n            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n            # nn.BatchNorm2d(192),\n            nn.LeakyReLU(0.1, inplace=True)\n        )\n        pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        conv3 = nn.Sequential(\n            nn.Conv2d(192, 128, kernel_size=1),\n            # nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.1, inplace=True),\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            # nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.1, inplace=True),\n            nn.Conv2d(256, 256, kernel_size=1),\n            # nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.1, inplace=True),\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            # nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.1, inplace=True)\n        )\n        pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        conv4_part = nn.Sequential(\n            nn.Conv2d(512, 256, kernel_size=1),\n            # nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.1, inplace=True),\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            # nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.1, inplace=True)\n        )\n        conv4_modules = []\n        for _ in range(4):\n            conv4_modules.append(conv4_part)\n        conv4 = nn.Sequential(\n            *conv4_modules,\n            nn.Conv2d(512, 512, kernel_size=1),\n            # nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.1, inplace=True),\n            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n            # nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.1, inplace=True)\n        )\n        pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n        conv5 = nn.Sequential(\n            nn.Conv2d(1024, 512, kernel_size=1),\n            # nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.1, inplace=True),\n            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n            # nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.1, inplace=True),\n            nn.Conv2d(1024, 512, kernel_size=1),\n            # nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.1, inplace=True),\n            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n            # nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.1, inplace=True)\n        )\n        self.net = nn.Sequential(conv1, pool1, conv2, pool2, conv3, pool3, conv4, pool4, conv5)\n\n    def forward(self, X):\n        return self.net(X)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:21:30.014199Z","iopub.execute_input":"2024-04-27T15:21:30.014927Z","iopub.status.idle":"2024-04-27T15:21:30.030838Z","shell.execute_reply.started":"2024-04-27T15:21:30.014896Z","shell.execute_reply":"2024-04-27T15:21:30.029800Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class Yolo(nn.Module):\n    def __init__(self, backbone: YoloBackbone = YoloBackbone(), backbone_out_channels=1024, n_classes=80):\n        self.n_classes = n_classes\n        super(Yolo, self).__init__()\n        self.backbone = backbone\n        self.head = nn.Sequential(\n            nn.Conv2d(backbone_out_channels, 1024, kernel_size=3, padding=1),\n            # nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.1, inplace=True),\n            nn.Conv2d(1024, 1024, kernel_size=3, padding=1, stride=2),\n            # nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.1, inplace=True),\n            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n            # nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.1, inplace=True),\n            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n            # nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.1, inplace=True),\n            nn.Flatten(),\n            nn.Linear(7*7*1024, 4096),\n            nn.Dropout(0.5),\n            nn.LeakyReLU(0.1, inplace=True),\n            nn.Linear(4096, 7 * 7 *(2 * 5 + self.n_classes)),\n            nn.Sigmoid(),\n            nn.Unflatten(1, (7, 7, (2 * 5 + self.n_classes)))\n        )\n        self.net = nn.Sequential(self.backbone, self.head)\n\n    def forward(self, X):\n        return self.net(X)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:21:32.809822Z","iopub.execute_input":"2024-04-27T15:21:32.810583Z","iopub.status.idle":"2024-04-27T15:21:33.003824Z","shell.execute_reply.started":"2024-04-27T15:21:32.810552Z","shell.execute_reply":"2024-04-27T15:21:33.002995Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Utilis","metadata":{}},{"cell_type":"markdown","source":"#### IoU","metadata":{}},{"cell_type":"code","source":"def is_intersect(self, boxA, boxB):\n    if boxA[0] > boxB[2]:\n        return False  \n    if boxA[1] > boxB[3]:\n        return False  \n    if boxA[2] < boxB[0]:\n        return False \n    if boxA[3] < boxB[1]:\n        return False  \n    return True\n\n\ndef get_intersection(self, boxA, boxB):\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    return (xB - xA + 1) * (yB - yA + 1)\n\n\ndef get_union(self, boxA, boxB):\n    area_A = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    area_B = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n    return area_A + area_B\n\n\ndef transform_x1y1wh_to_x1y1x2y2(self, box):\n    x1 = round(box[0], 2)\n    y1 = round(box[1], 2)\n    x2 = round(box[0] + box[2], 2)\n    y2 = round(box[1] + box[3], 2)\n    return [x1, y1, x2, y2]\n\n\ndef get_IoU(self, boxA, boxB):\n    # x1y1wh -> x1y1x2y2\n    boxA = self.transform_x1y1wh_to_x1y1x2y2(boxA)\n    boxB = self.transform_x1y1wh_to_x1y1x2y2(boxB)\n    if self.is_intersect(boxA, boxB) is False:\n        return 0\n    inter = self.get_intersection(boxA, boxB)\n    union = self.get_union(boxA, boxB)\n    iou = inter / (union - inter)\n    return iou","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:21:35.273859Z","iopub.execute_input":"2024-04-27T15:21:35.274593Z","iopub.status.idle":"2024-04-27T15:21:35.286370Z","shell.execute_reply.started":"2024-04-27T15:21:35.274561Z","shell.execute_reply":"2024-04-27T15:21:35.285219Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"#### Loss","metadata":{}},{"cell_type":"code","source":"def yolo_loss(yhat, y, lambda_coord=5, lambda_noobj=0.5, n_classes=80):\n    \"\"\"\n    Args:\n        yhat: [#, 7, 7, 30]\n        y: [#, 7, 7, 30]\n    Returns:\n        loss: [#]\n    \"\"\"\n    with torch.no_grad():\n        # arrange cell xidx, yidx\n        # [7, 7]\n        cell_xidx = (torch.arange(49) % 7).reshape(7, 7)\n        cell_yidx = (torch.div(torch.arange(49), 7, rounding_mode='floor')).reshape(7, 7)\n        # transform to [7, 7, 2]\n        cell_xidx.unsqueeze_(-1)\n        cell_yidx.unsqueeze_(-1)\n        cell_xidx.expand(7, 7, 2)\n        cell_yidx.expand(7, 7, 2)\n        # move to device\n        cell_xidx = cell_xidx.to(yhat.device)\n        cell_yidx = cell_yidx.to(yhat.device)\n\n    def calc_coord(val):\n        with torch.no_grad():\n            # transform cell relative coordinates to image relative coordinates\n            x = (val[..., 0] + cell_xidx) / 7.0\n            y = (val[..., 1] + cell_yidx) / 7.0\n\n            return (x - val[..., 2] / 2.0,\n                x + val[..., 2] / 2.0,\n                y - val[..., 3] / 2.0,\n                y + val[..., 3] / 2.0)\n\n    y_area = y[..., :10].reshape(-1, 7, 7, 2, 5)\n    yhat_area = yhat[..., :10].reshape(-1, 7, 7, 2, 5)\n\n    y_class = y[..., 10:].reshape(-1, 7, 7, n_classes)\n    yhat_class = yhat[..., 10:].reshape(-1, 7, 7, n_classes)\n\n    with torch.no_grad():\n        # calculate IoU\n        x_min, x_max, y_min, y_max = calc_coord(y_area)\n        x_min_hat, x_max_hat, y_min_hat, y_max_hat = calc_coord(yhat_area)\n\n        wi = torch.min(x_max, x_max_hat) - torch.max(x_min, x_min_hat)\n        wi = torch.max(wi, torch.zeros_like(wi))\n        hi = torch.min(y_max, y_max_hat) - torch.max(y_min, y_min_hat)\n        hi = torch.max(hi, torch.zeros_like(hi))\n\n        intersection = wi * hi\n        union = (x_max - x_min) * (y_max - y_min) + (x_max_hat - x_min_hat) * (y_max_hat - y_min_hat) - intersection\n        iou = intersection / (union + 1e-6) # add epsilon to avoid nan\n\n        _, res = iou.max(dim=3, keepdim=True)\n\n    # [#, 7, 7, 5]\n    # responsible bounding box (having higher IoU)\n    yhat_res = torch.take_along_dim(yhat_area, res.unsqueeze(3), 3).squeeze_(3)\n    y_res = y_area[..., 0, :5]\n\n    with torch.no_grad():\n        # calculate indicator matrix\n        have_obj = y_res[..., 4] > 0\n        no_obj = ~have_obj\n\n    return ((lambda_coord * ( # coordinate loss\n          (y_res[..., 0] - yhat_res[..., 0]) ** 2 # X\n        + (y_res[..., 1] - yhat_res[..., 1]) ** 2 # Y\n        + (torch.sqrt(y_res[..., 2]) - torch.sqrt(yhat_res[..., 2])) ** 2  # W\n        + (torch.sqrt(y_res[..., 3]) - torch.sqrt(yhat_res[..., 3])) ** 2) # H\n        # confidence\n        + (y_res[..., 4] - yhat_res[..., 4]) ** 2\n        # class\n        + ((y_class - yhat_class) ** 2).sum(dim=3)) * have_obj\n        # noobj\n        + ((y_area[..., 0, 4] - yhat_area[..., 0, 4]) ** 2 + \\\n        (y_area[..., 1, 4] - yhat_area[..., 1, 4]) ** 2) * no_obj * lambda_noobj).sum(dim=(1, 2))","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:21:37.743700Z","iopub.execute_input":"2024-04-27T15:21:37.744257Z","iopub.status.idle":"2024-04-27T15:21:37.763702Z","shell.execute_reply.started":"2024-04-27T15:21:37.744220Z","shell.execute_reply":"2024-04-27T15:21:37.762583Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"#### NMS","metadata":{}},{"cell_type":"code","source":"def nms(pred, threshold=0.5):\n\n    with torch.no_grad():\n        pred = pred.reshape((-1, 30))\n        nms_data = [[] for _ in range(80)]\n        for i in range(pred.shape[0]):\n            cell = pred[i]\n            score, idx = torch.max(cell[10:30], dim=0)\n            idx = idx.item()\n            x, y, w, h, iou = cell[0:5].cpu().numpy()\n\n            nms_data[idx].append([i, x, y, w, h, iou, score.item()])\n            x, y, w, h, iou = cell[5:10].cpu().numpy()\n            nms_data[idx].append([i, x, y, w, h, iou, score.item()])\n\n        ret = torch.zeros_like(pred)\n        flag = torch.zeros(pred.shape[0], dtype=torch.bool)\n        for c in range(80):\n            c_nms_data = np.array(nms_data[c])\n\n            keep_index = _nms(c_nms_data, threshold)\n            keeps = c_nms_data[keep_index]\n\n            for keep in keeps:\n                i, x, y, w, h, iou, score = keep\n                i = int(i)\n\n                last_score, _ = torch.max(ret[i][10:30], dim=0)\n                last_iou = ret[i][4]\n\n                if score * iou > last_score * last_iou:\n                    flag[i] = False\n                if flag[i]: continue\n\n                ret[i][0:5] = torch.tensor([x, y, w, h, iou])\n                ret[i][10:30] = 0\n                ret[i][10 + c] = score\n\n                flag[i] = True\n\n        return ret\n    \n    \ndef _nms(data, threshold):\n\n    if len(data) == 0:\n        return []\n\n    cell_idx = data[:, 0]\n    x = data[:, 1]\n    y = data[:, 2]\n    xidx = cell_idx % 7\n    yidx = cell_idx // 7\n    x = (x + xidx) / 7.0\n    y = (y + yidx) / 7.0\n    w = data[:, 3]\n    h = data[:, 4]\n    x1 = x - w / 2\n    y1 = y - h / 2\n    x2 = x + w / 2\n    y2 = y + h / 2\n\n    score_area = data[:, 5]\n\n    areas = w * h\n\n    order = score_area.argsort()[::-1]\n    keep = []\n\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1)\n        h = np.maximum(0.0, yy2 - yy1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= threshold)[0]\n        order = order[inds + 1]\n\n    return keep","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:30:02.206067Z","iopub.execute_input":"2024-04-27T20:30:02.206973Z","iopub.status.idle":"2024-04-27T20:30:02.226938Z","shell.execute_reply.started":"2024-04-27T20:30:02.206940Z","shell.execute_reply":"2024-04-27T20:30:02.226077Z"},"trusted":true},"execution_count":184,"outputs":[]},{"cell_type":"markdown","source":"#### mAP","metadata":{}},{"cell_type":"code","source":"def calculate_mAP(self, classAP_data):\n    AP_50_per_class, PR_50_pts_per_class = {}, {}\n    (\n        num_true_per_class,\n        num_positive_per_class,\n        num_TP_50_per_class,\n        num_FP_50_per_class,\n    ) = ({}, {}, {}, {})\n    mAP_50, mAP_75, mAP_5095 = 0, 0, 0\n    valid_num_classes = 0 + 1e-8\n\n    for res in classAP_data:\n        if res[\"total_positive\"] > 0:\n            valid_num_classes += 1\n            AP_50_per_class[res[\"class\"]] = res[\"AP_50\"]\n            PR_50_pts_per_class[res[\"class\"]] = {\n                \"mprec\": res[\"prec_50\"],\n                \"mrec\": res[\"rec_50\"],\n            }\n            num_true_per_class[res[\"class\"]] = res[\"total_true\"]\n            num_positive_per_class[res[\"class\"]] = res[\"total_positive\"]\n            num_TP_50_per_class[res[\"class\"]] = res[\"total_TP_50\"]\n            num_FP_50_per_class[res[\"class\"]] = res[\"total_FP_50\"]\n            mAP_50 += res[\"AP_50\"]\n            mAP_75 += res[\"AP_75\"]\n            mAP_5095 += res[\"AP_5095\"]\n\n    mAP_50 /= valid_num_classes\n    mAP_75 /= valid_num_classes\n    mAP_5095 /= valid_num_classes\n\n    res = {\n        \"AP_50_PER_CLASS\": AP_50_per_class,\n        \"PR_50_PTS_PER_CLASS\": PR_50_pts_per_class,\n        \"NUM_TRUE_PER_CLASS\": num_true_per_class,\n        \"NUM_POSITIVE_PER_CLASS\": num_positive_per_class,\n        \"NUM_TP_50_PER_CLASS\": num_TP_50_per_class,\n        \"NUM_FP_50_PER_CLASS\": num_FP_50_per_class,\n        \"mAP_50\": round(mAP_50, 4),\n        \"mAP_75\": round(mAP_75, 4),\n        \"mAP_5095\": round(mAP_5095, 4),\n    }\n    return res","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:21:48.002719Z","iopub.execute_input":"2024-04-27T15:21:48.003606Z","iopub.status.idle":"2024-04-27T15:21:48.013383Z","shell.execute_reply.started":"2024-04-27T15:21:48.003570Z","shell.execute_reply":"2024-04-27T15:21:48.012267Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def mean_average_precision(\n    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=5\n):\n    \"\"\"\n    Calculates mean average precision\n    Parameters:\n        pred_boxes (list): list of lists containing all bboxes with each bboxes\n        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n        true_boxes (list): Similar as pred_boxes except all the correct ones\n        iou_threshold (float): threshold where predicted bboxes is correct\n        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n        num_classes (int): number of classes\n    Returns:\n        float: mAP value across all classes given a specific IoU threshold\n    \"\"\"\n\n    # list storing all AP for respective classes\n    average_precisions = []\n\n    # used for numerical stability later on\n    epsilon = 1e-6\n\n    for c in range(num_classes):\n        detections = []\n        ground_truths = []\n\n        # Go through all predictions and targets,\n        # and only add the ones that belong to the\n        # current class c\n        for detection in pred_boxes:\n            if detection[1] == c:\n                detections.append(detection)\n\n        for true_box in true_boxes:\n            if true_box[1] == c:\n                ground_truths.append(true_box)\n\n        # find the amount of bboxes for each training example\n        # Counter here finds how many ground truth bboxes we get\n        # for each training example, so let's say img 0 has 3,\n        # img 1 has 5 then we will obtain a dictionary with:\n        # amount_bboxes = {0:3, 1:5}\n        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n\n        # We then go through each key, val in this dictionary\n        # and convert to the following (w.r.t same example):\n        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n        for key, val in amount_bboxes.items():\n            amount_bboxes[key] = torch.zeros(val)\n\n        # sort by box probabilities which is index 2\n        detections.sort(key=lambda x: x[2], reverse=True)\n        TP = torch.zeros((len(detections)))\n        FP = torch.zeros((len(detections)))\n        total_true_bboxes = len(ground_truths)\n\n        # If none exists for this class then we can safely skip\n        if total_true_bboxes == 0:\n            continue\n\n        for detection_idx, detection in enumerate(detections):\n            # Only take out the ground_truths that have the same\n            # training idx as detection\n            ground_truth_img = [\n                bbox for bbox in ground_truths if bbox[0] == detection[0]\n            ]\n\n            num_gts = len(ground_truth_img)\n            best_iou = 0\n\n            for idx, gt in enumerate(ground_truth_img):\n                iou = intersection_over_union(\n                    torch.tensor(detection[3:]),\n                    torch.tensor(gt[3:]),\n                    box_format=box_format,\n                )\n\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_idx = idx\n\n            if best_iou > iou_threshold:\n                # only detect ground truth detection once\n                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n                    # true positive and add this bounding box to seen\n                    TP[detection_idx] = 1\n                    amount_bboxes[detection[0]][best_gt_idx] = 1\n                else:\n                    FP[detection_idx] = 1\n\n            # if IOU is lower then the detection is a false positive\n            else:\n                FP[detection_idx] = 1\n\n        TP_cumsum = torch.cumsum(TP, dim=0)\n        FP_cumsum = torch.cumsum(FP, dim=0)\n        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n        precisions = torch.cat((torch.tensor([1]), precisions))\n        recalls = torch.cat((torch.tensor([0]), recalls))\n        # torch.trapz for numerical integration\n        average_precisions.append(torch.trapz(precisions, recalls))\n\n    return sum(average_precisions) / (len(average_precisions) + 1e-6)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:21:49.435597Z","iopub.execute_input":"2024-04-27T15:21:49.436357Z","iopub.status.idle":"2024-04-27T15:21:49.453654Z","shell.execute_reply.started":"2024-04-27T15:21:49.436321Z","shell.execute_reply":"2024-04-27T15:21:49.452586Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"#### Weights initialization","metadata":{}},{"cell_type":"code","source":"def weight_init(m):\n    '''\n    Usage:\n        model = Model()\n        model.apply(weight_init)\n    '''\n    if isinstance(m, nn.Conv1d):\n        init.normal_(m.weight.data)\n        if m.bias is not None:\n            init.normal_(m.bias.data)\n    elif isinstance(m, nn.Conv2d):\n        init.xavier_normal_(m.weight.data)\n        if m.bias is not None:\n            init.normal_(m.bias.data)\n    elif isinstance(m, nn.Conv3d):\n        init.xavier_normal_(m.weight.data)\n        if m.bias is not None:\n            init.normal_(m.bias.data)\n    elif isinstance(m, nn.ConvTranspose1d):\n        init.normal_(m.weight.data)\n        if m.bias is not None:\n            init.normal_(m.bias.data)\n    elif isinstance(m, nn.ConvTranspose2d):\n        init.xavier_normal_(m.weight.data)\n        if m.bias is not None:\n            init.normal_(m.bias.data)\n    elif isinstance(m, nn.ConvTranspose3d):\n        init.xavier_normal_(m.weight.data)\n        if m.bias is not None:\n            init.normal_(m.bias.data)\n    elif isinstance(m, nn.BatchNorm1d):\n        init.normal_(m.weight.data, mean=1, std=0.02)\n        init.constant_(m.bias.data, 0)\n    elif isinstance(m, nn.BatchNorm2d):\n        init.normal_(m.weight.data, mean=1, std=0.02)\n        init.constant_(m.bias.data, 0)\n    elif isinstance(m, nn.BatchNorm3d):\n        init.normal_(m.weight.data, mean=1, std=0.02)\n        init.constant_(m.bias.data, 0)\n    elif isinstance(m, nn.Linear):\n        init.xavier_normal_(m.weight.data)\n        init.normal_(m.bias.data)\n    elif isinstance(m, nn.LSTM):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data)\n    elif isinstance(m, nn.LSTMCell):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data)\n    elif isinstance(m, nn.GRU):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data)\n    elif isinstance(m, nn.GRUCell):\n        for param in m.parameters():\n            if len(param.shape) >= 2:\n                init.orthogonal_(param.data)\n            else:\n                init.normal_(param.data)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:21:52.238200Z","iopub.execute_input":"2024-04-27T15:21:52.238897Z","iopub.status.idle":"2024-04-27T15:21:52.256155Z","shell.execute_reply.started":"2024-04-27T15:21:52.238865Z","shell.execute_reply":"2024-04-27T15:21:52.255204Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"#### Other","metadata":{}},{"cell_type":"code","source":"class Accumulator(object):\n    \"\"\"\n    Sum a list of numbers over time\n    from: https://github.com/dsgiitr/d2l-pytorch/blob/master/d2l/base.py\n    \"\"\"\n    def __init__(self, n):\n        self.data = [0.0] * n\n    def add(self, *args):\n        self.data = [a + b for a, b in zip(self.data, args)]\n    def reset(self):\n        self.data = [0] * len(self.data)\n    def __getitem__(self, i):\n        return self.data[i]\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:21:54.791854Z","iopub.execute_input":"2024-04-27T15:21:54.792858Z","iopub.status.idle":"2024-04-27T15:21:54.799569Z","shell.execute_reply.started":"2024-04-27T15:21:54.792805Z","shell.execute_reply":"2024-04-27T15:21:54.798531Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class Timer(object):\n    \"\"\"Record multiple running times.\"\"\"\n    def __init__(self):\n        self.times = []\n        self.start()\n\n    def start(self):\n        \"\"\"Start the timer\"\"\"\n        self.start_time = time.time()\n\n    def stop(self):\n        \"\"\"Stop the timer and record the time in a list\"\"\"\n        self.times.append(time.time() - self.start_time)\n        return self.times[-1]\n\n    def avg(self):\n        \"\"\"Return the average time\"\"\"\n        return sum(self.times)/len(self.times)\n\n    def sum(self):\n        \"\"\"Return the sum of time\"\"\"\n        return sum(self.times)\n\n    def cumsum(self):\n        \"\"\"Return the accumuated times\"\"\"\n        return np.array(self.times).cumsum().tolist()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:21:56.173740Z","iopub.execute_input":"2024-04-27T15:21:56.174177Z","iopub.status.idle":"2024-04-27T15:21:56.181626Z","shell.execute_reply.started":"2024-04-27T15:21:56.174143Z","shell.execute_reply":"2024-04-27T15:21:56.180555Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"def train(net, train_iter, test_iter, num_epochs, lr, momentum=0.9, weight_decay=5e-4, accum_batch_num=1, save_path='./chkpt', load=None, load_epoch=-1, pretrained=False):\n    '''\n    Train net work. Some notes for load & load_epoch:\n    :param load: the file of model weights to load\n    :param load_epoch: num of epoch already completed (minus 1). should be the same with the number in auto-saved file name.\n    '''\n\n    def print_and_log(msg, log_file):\n        print(msg)\n        with open(log_file, 'a', encoding='utf8') as f:\n            f.write(msg + '\\n')\n\n    def update_lr(opt, lr):\n        for param_group in opt.param_groups:\n            param_group['lr'] = lr\n\n    os.makedirs(save_path, exist_ok=True)\n    log_file = os.path.join(save_path, f'log-{time.time_ns()}.txt')\n\n    if load:\n        net.load_state_dict(torch.load(load))\n    elif pretrained:\n        net.head.apply(weight_init)\n    else:\n        # init params\n        net.apply(weight_init)\n\n    if not torch.cuda.is_available():\n        net = net.to(torch.device('cpu'))\n        devices = [torch.device('cpu')]\n    else:\n        net = net.to(torch.device('cuda'))\n        devices = [torch.device('cuda')]\n\n    # define optimizer\n    if isinstance(lr, float):\n        tlr = lr\n    else: tlr = 0.001\n\n    optimizer = torch.optim.SGD(net.parameters(), lr=tlr, momentum=momentum, weight_decay=weight_decay)\n\n    # visualization\n\n    num_batches = len(train_iter)\n    # train\n    for epoch in range(num_epochs - load_epoch - 1):\n        # adjust true epoch number according to pre_load\n        epoch = epoch + load_epoch + 1\n\n        # define metrics: train loss, sample count\n        metrics = Accumulator(2)\n        # define timer\n        timer = Timer()\n\n        # train\n        net.train()\n\n        # set batch accumulator\n        accum_cnt = 0\n        accum = 0\n        loop = tqdm(train_iter, leave=True)\n\n        for batch_idx, (X, y) in enumerate(loop):\n            timer.start()\n\n            X, y = X.to(devices[0]), y.to(devices[0])\n            yhat = net(X)\n            \n            loss_val = yolo_loss(yhat, y)\n            # print(loss_val)\n\n            # backward to accumulate gradients\n            loss_val.sum().backward()\n            # step\n            optimizer.step()\n            # clear\n            optimizer.zero_grad()\n\n\n            # update metrics\n            with torch.no_grad():\n                metrics.add(loss_val.sum().cpu(), X.shape[0])\n            train_l = м[0] / metrics[1]\n\n            timer.stop()\n\n            # log & visualization\n            if (batch_idx + 1) % (num_batches // 5) == 0 or batch_idx == num_batches - 1:\n                print_and_log(\"epoch: %d, batch: %d / %d, loss: %.4f, time: %.4f\" % (epoch, batch_idx + 1, num_batches, train_l.item(), timer.sum()), log_file)\n\n        # redefine metrics: test loss, test sample count\n        metrics = Accumulator(2)\n        # redefine timer\n        timer = Timer()\n        # test\n        net.eval()\n\n        with torch.no_grad():\n            timer.start()\n\n            for batch in test_iter:\n                X, y = batch\n                X, y = X.to(devices[0]), y.to(devices[0])\n                yhat = net(X)\n\n                loss_val = yolo_loss(yhat, y)\n                metrics.add(loss_val.sum().cpu(), X.shape[0])\n\n            timer.stop()\n\n            test_l = metrics[0] / metrics[1]\n            print_and_log(\"epoch: %d, test loss: %.4f, time: %.4f\" % (epoch + 1, test_l.item(), timer.sum()), log_file)\n\n        # save model\n        if epoch % 5 == 0:\n            torch.save(net.state_dict(), os.path.join(save_path, f'./{time.time_ns()}-epoch-{epoch}.pth'))","metadata":{"execution":{"iopub.status.busy":"2024-04-27T15:22:00.773926Z","iopub.execute_input":"2024-04-27T15:22:00.774811Z","iopub.status.idle":"2024-04-27T15:22:00.794221Z","shell.execute_reply.started":"2024-04-27T15:22:00.774778Z","shell.execute_reply":"2024-04-27T15:22:00.793307Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"dataset = load_coco(15000, data_dir)\ndataset.persistent = True\n# dataset = fo.Dataset.from_dir(dataset_dir='/kaggle/working/', dataset_type=fo.types.COCODetectionDataset,)\nclasses = dataset.distinct(\"ground_truth.detections.label\")\ntrain_data = dataset.match_tags(\"train\")\ntest_data = dataset.match_tags(\"test\")\nval_data = dataset.match_tags(\"validation\")\ntrain_dataset = FiftyOneTorchDataset(train_data, transforms=Transformtsz(resize=(448, 448)), classes=classes)\nval_dataset_test = FiftyOneTorchDataset(val_data, transforms=Transformtsz(resize=(448, 448)), classes=classes)\ntest_dataset_test = FiftyOneTorchDataset(test_data, transforms=Transformtsz(resize=(448, 448)), classes=classes)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=False, collate_fn=collate)#, sampler=train_sampler)\nval_loader = torch.utils.data.DataLoader(val_dataset_test, batch_size=8, shuffle=False, collate_fn=collate)#, sampler=train_sampler)\ntest_loader = torch.utils.data.DataLoader(test_dataset_test, batch_size=8, shuffle=False, collate_fn=collate)#, sampler=train_sampler)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T16:02:48.760107Z","iopub.execute_input":"2024-04-27T16:02:48.760544Z","iopub.status.idle":"2024-04-27T16:06:15.905532Z","shell.execute_reply.started":"2024-04-27T16:02:48.760503Z","shell.execute_reply":"2024-04-27T16:06:15.904370Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Downloading split 'train' to '/kaggle/working/data/train' if necessary\nFound annotations at '/kaggle/working/data/raw/instances_train2017.json'\nSufficient images already downloaded\nExisting download of split 'train' is sufficient\nDownloading split 'validation' to '/kaggle/working/data/validation' if necessary\nFound annotations at '/kaggle/working/data/raw/instances_val2017.json'\nOnly found 5000 (<15000) samples matching your requirements\nSufficient images already downloaded\nExisting download of split 'validation' is sufficient\nDownloading split 'test' to '/kaggle/working/data/test' if necessary\nFound test info at '/kaggle/working/data/raw/image_info_test2017.json'\nSufficient images already downloaded\nExisting download of split 'test' is sufficient\nLoading 'coco-2017' split 'train'\n 100% |█████████████| 15000/15000 [1.9m elapsed, 0s remaining, 114.4 samples/s]      \nLoading 'coco-2017' split 'validation'\n 100% |███████████████| 5000/5000 [38.0s elapsed, 0s remaining, 138.6 samples/s]      \nLoading 'coco-2017' split 'test'\n 100% |█████████████| 15000/15000 [6.6s elapsed, 0s remaining, 2.4K samples/s]      \nDataset 'coco-2017-train-validation-test-15000' created\n","output_type":"stream"}]},{"cell_type":"code","source":"#resnet18 = torchvision.models.resnet18(pretrained=True)\n# net = Yolo() # classical YoloV1 with our backbone\n# resnet 18 backbone\n# remove avg pool and fc\nresnet18 = torchvision.models.resnet18(weights='ResNet18_Weights.IMAGENET1K_V1')\nbackbone = nn.Sequential(*list(resnet18.children())[:-2])\nfor param in backbone.parameters():\n    param.requires_grad = False\nnet = Yolo(backbone, backbone_out_channels=512)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T14:35:50.902776Z","iopub.execute_input":"2024-04-27T14:35:50.903536Z","iopub.status.idle":"2024-04-27T14:35:53.744876Z","shell.execute_reply.started":"2024-04-27T14:35:50.903502Z","shell.execute_reply":"2024-04-27T14:35:53.744014Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"code","source":"train(net, train_iter=train_loader, test_iter=test_loader, num_epochs=20, lr=0.0001)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T20:49:07.154332Z","iopub.execute_input":"2024-04-26T20:49:07.154687Z","iopub.status.idle":"2024-04-27T00:25:27.842399Z","shell.execute_reply.started":"2024-04-26T20:49:07.154654Z","shell.execute_reply":"2024-04-27T00:25:27.841423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r coco15k.zip kaggle/working/data\ndisplay(FileLink('coco15k.zip'))","metadata":{"execution":{"iopub.status.busy":"2024-04-27T00:28:17.451282Z","iopub.execute_input":"2024-04-27T00:28:17.451695Z","iopub.status.idle":"2024-04-27T00:28:18.907566Z","shell.execute_reply.started":"2024-04-27T00:28:17.451666Z","shell.execute_reply":"2024-04-27T00:28:18.906158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"resnet18 = torchvision.models.resnet18(weights='ResNet18_Weights.IMAGENET1K_V1')\nbackbone = nn.Sequential(*list(resnet18.children())[:-2])\nnet = Yolo(backbone, backbone_out_channels=512)\nnet.load_state_dict(torch.load('/kaggle/working/chkpt/1714178438389879385-epoch-20.pth'))\nnet.eval()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:52:23.346965Z","iopub.execute_input":"2024-04-27T20:52:23.347343Z","iopub.status.idle":"2024-04-27T20:52:27.465226Z","shell.execute_reply.started":"2024-04-27T20:52:23.347317Z","shell.execute_reply":"2024-04-27T20:52:27.464245Z"},"trusted":true},"execution_count":191,"outputs":[{"execution_count":191,"output_type":"execute_result","data":{"text/plain":"Yolo(\n  (backbone): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (head): Sequential(\n    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (3): LeakyReLU(negative_slope=0.1, inplace=True)\n    (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): LeakyReLU(negative_slope=0.1, inplace=True)\n    (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): LeakyReLU(negative_slope=0.1, inplace=True)\n    (8): Flatten(start_dim=1, end_dim=-1)\n    (9): Linear(in_features=50176, out_features=4096, bias=True)\n    (10): Dropout(p=0.5, inplace=False)\n    (11): LeakyReLU(negative_slope=0.1, inplace=True)\n    (12): Linear(in_features=4096, out_features=4410, bias=True)\n    (13): Sigmoid()\n    (14): Unflatten(dim=1, unflattened_size=(7, 7, 90))\n  )\n  (net): Sequential(\n    (0): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (4): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (5): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (6): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (downsample): Sequential(\n            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (7): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (1): Sequential(\n      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n      (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (3): LeakyReLU(negative_slope=0.1, inplace=True)\n      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (5): LeakyReLU(negative_slope=0.1, inplace=True)\n      (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (7): LeakyReLU(negative_slope=0.1, inplace=True)\n      (8): Flatten(start_dim=1, end_dim=-1)\n      (9): Linear(in_features=50176, out_features=4096, bias=True)\n      (10): Dropout(p=0.5, inplace=False)\n      (11): LeakyReLU(negative_slope=0.1, inplace=True)\n      (12): Linear(in_features=4096, out_features=4410, bias=True)\n      (13): Sigmoid()\n      (14): Unflatten(dim=1, unflattened_size=(7, 7, 90))\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"class RawDataforTest(torch.utils.data.Dataset):\n    \n    def __init__(self, fiftyone_dataset, transforms=None, gt_field=\"ground_truth\", classes=None,):\n        \n        self.samples = fiftyone_dataset\n        self.transforms = transforms\n        self.gt_field = gt_field\n\n        self.img_paths = self.samples.values(\"filepath\")\n\n        self.classes = classes\n        if not self.classes:\n            # Get list of distinct labels that exist in the view\n            self.classes = self.samples.distinct(\n                \"%s.detections.label\" % gt_field\n            )\n\n        if self.classes[0] != \"background\":\n            self.classes = [\"background\"] + self.classes\n\n        self.labels_map_rev = {c: i for i, c in enumerate(self.classes)}\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        \n        img_path = self.img_paths[idx]\n        sample = self.samples[img_path]\n        metadata = sample.metadata\n        img = Image.open(img_path).convert(\"RGB\")\n        width, height = img.size\n\n        if sample[self.gt_field] is not None:\n            detections = sample[self.gt_field].detections\n            for det in detections:\n                category_id = self.labels_map_rev[det.label]\n                coco_obj = fouc.COCOObject.from_label(\n                    det, metadata, category_id=category_id,\n                )   \n                x, y, w, h = coco_obj.bbox\n                \n                # update labels from absolute to relative\n                h, w = float(h), float(w)\n\n                ret_targets = []\n                ret_targets.append({\n                        'xmin': float(x) / w,\n                        'ymin': float(y) / h,\n                        'xmax': float(x+w) / w,\n                        'ymax': float(x+h) / h,\n                        'category': category_id,\n                })\n                \n                \n            img = torchvision.transforms.functional.resize(img, (448, 448))\n            #img = torchvision.transforms.ToTensor(img)\n            img = torchvision.transforms.functional.to_tensor(img)\n            #if self.transforms is not None:\n                #img, target = self.transforms(img, target)\n        \n            return img, json.dumps(ret_targets)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T21:52:37.485122Z","iopub.execute_input":"2024-04-27T21:52:37.486152Z","iopub.status.idle":"2024-04-27T21:52:37.499204Z","shell.execute_reply.started":"2024-04-27T21:52:37.486116Z","shell.execute_reply":"2024-04-27T21:52:37.498245Z"},"trusted":true},"execution_count":206,"outputs":[]},{"cell_type":"code","source":"from enum import Enum\nimport json\n\n\nclass InterpolationMethod(Enum):\n    Interpolation_11 = 1\n    Interpolation_101 = 2\n\n\nclass CalculationMetrics():\n    def __init__(self, IoU: float, confidence: float, mustbe_FP: bool):#, is_difficult: bool):\n\n        self.IoU = IoU\n        self.confidence = confidence\n        self.mustbe_FP = mustbe_FP\n        #self.is_difficult = is_difficult\n\n\ndef compare_metrics(metrics1: CalculationMetrics, metrics2: CalculationMetrics):\n    if metrics1.confidence == metrics2.confidence:\n        return metrics2.IoU - metrics1.IoU\n    return metrics2.confidence - metrics1.confidence\n\n\nclass ObjectDetectionMetricsCalculator():\n\n    def __init__(self, num_classes: int, confidence_thres: float):\n\n        # initialize data\n        self.data = [{\"data\": [], \"detection\": 0, \"truth\": 0} for _ in range(num_classes)]\n        self.confidence_thres = confidence_thres\n\n\n    def add_image_data(self, pred: torch.Tensor, truth: str):\n\n        pred = pred.reshape(-1, 30)\n        truth = json.loads(truth)\n\n        choose_truth_index = [None for _ in range(pred.shape[0])]\n        iou = [0 for _ in range(pred.shape[0])]\n\n        for i in range(pred.shape[0]):\n            score, cat = pred[i][10:30].max(dim=0)\n            confidence = pred[i][4]\n            # filter by confidence threshold\n            if confidence * score < self.confidence_thres: continue\n            \n            x, y, w, h = pred[i][0:4]\n            # calculate cell index\n            xidx = i % 7\n            yidx = i // 7\n            # transform cell relative coordinates to image relative coordinates\n            xhat = (x + xidx) / 7.0\n            yhat = (y + yidx) / 7.0\n\n            xmin_hat = xhat - w / 2\n            xmax_hat = xhat + w / 2\n            ymin_hat = yhat - h / 2\n            ymax_hat = yhat + h / 2\n\n            for j in range(len(truth)):\n                bbox = truth[j]\n                # judge whether is same class\n                if cat != bbox['category']: continue\n                # calculate IoU\n                xmin, ymin, xmax, ymax = bbox['xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax']\n                wi = min(xmax, xmax_hat) - max(xmin, xmin_hat)\n                wi = max(wi, 0)\n                hi = min(ymax, ymax_hat) - max(ymin, ymin_hat)\n                hi = max(hi, 0)\n                intersection = wi * hi\n                union = (xmax - xmin) * (ymax - ymin) + (xmax_hat - xmin_hat) * (ymax_hat - ymin_hat) - intersection\n                this_iou = intersection / (union + 1e-6)\n                # determine whether to choose this ground truth\n                if iou[i] is None: choose = True\n                elif iou[i] < this_iou: choose = True\n                else: choose = False\n                # if choose, assign value\n                if choose:\n                    iou[i] = this_iou\n                    choose_truth_index[i] = j\n        # init a bool array for judging mustbe_FP later\n        truth_chosen = [False for _ in range(len(truth))]\n        # sort according to IoU\n        sort_idx = np.argsort(iou)[::-1]\n        # add into metrics\n        for i in sort_idx:\n            score, cat = pred[i][10:30].max(dim=0)\n            confidence = pred[i][4]\n            # filter by confidence threshold\n            if confidence * score < self.confidence_thres: continue\n\n            truth_index = choose_truth_index[i]\n            if truth_index == None: \n                mustbe_FP = True\n                is_difficult = False\n            elif truth_chosen[truth_index]:\n                mustbe_FP = True\n                #is_difficult = truth[choose_truth_index[i]]['difficult']\n            else: \n                mustbe_FP = False\n                truth_chosen[choose_truth_index[i]] = True\n                #is_difficult = truth[choose_truth_index[i]]['difficult']\n\n            self.data[cat]['data'].append(CalculationMetrics(iou[i], float(confidence * score), mustbe_FP))#, is_difficult))\n\n            # update detection statistics\n            self.data[cat]['detection'] += 1\n        # update ground truth statistics\n        for bbox in truth:\n            #if bbox['difficult']: continue\n            self.data[bbox['category']]['truth'] += 1\n\n\n    def calculate_precision_recall(self, iou_thres: float, class_idx: int) -> list:\n\n        ret = []\n        # retrieve count\n        truth_cnt = self.data[class_idx]['truth']\n        # accumulated TP\n        acc_TP = 0\n        # accumulated difficult count\n        #acc_difficult = 0\n        # sort metrics by confidence\n        data = sorted(self.data[class_idx]['data'], key=cmp_to_key(compare_metrics))\n        for i, metrics in enumerate(data):\n            if metrics.IoU >= iou_thres and not metrics.mustbe_FP: #and not metrics.is_difficult:\n                acc_TP += 1\n            #if metrics.is_difficult:\n                #acc_difficult += 1\n            if i + 1 - acc_difficult > 0:\n                ret.append({\n                    'precision': acc_TP / (i + 1) #- acc_difficult),\n                    'recall': acc_TP / truth_cnt\n                })\n        \n        return ret\n\n\n    def calculate_average_precision(self, iou_thres: float, class_idx: int, itpl_option: InterpolationMethod) -> float:\n\n        prl = self.calculate_precision_recall(iou_thres=iou_thres, class_idx=class_idx)\n\n        if itpl_option == InterpolationMethod.Interpolation_11:\n            intp_pts = [0.1 * i for i in range(11)]\n        elif itpl_option == InterpolationMethod.Interpolation_101:\n            intp_pts = [0.01 * i for i in range(101)]\n        else:\n            raise Exception('Unknown Interpolation Method')\n\n        max_dict = {}\n        gmax = 0\n\n        for pr in prl[::-1]:\n            gmax = max(gmax, pr['precision'])\n            max_dict[pr['recall']] = gmax\n\n        if len(max_dict) < 1: return 0.\n\n        max_keys = max_dict.keys()\n        max_keys = sorted(max_keys)\n\n        key_ptr = len(max_keys) - 2\n        last_key = max_keys[-1]\n\n        AP = 0\n\n        for query in intp_pts[::-1]:\n            if key_ptr < 0:\n                if query > last_key:\n                    ans = 0\n                else:\n                    ans = max_dict[last_key]\n            else:\n                if query > last_key:\n                    ans = 0\n                elif query > max_keys[key_ptr]:\n                    ans = max_dict[last_key]\n                else:\n                    while key_ptr >= 0:\n                        if query > max_keys[key_ptr]:\n                            break\n                        last_key = max_keys[key_ptr]\n                        key_ptr -= 1\n                    ans = max_dict[last_key]\n            AP += ans\n\n        AP /= len(intp_pts)\n        return AP\n\n\n    def calculate_mAP(self, iou_thres: float, itpl_option: InterpolationMethod) -> float:\n        mAP = 0\n        for c in range(len(self.data)):\n            mAP += self.calculate_average_precision(iou_thres, c, itpl_option)\n        mAP /= len(self.data)\n\n        return mAP","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_and_draw_mAP(net, test_iter_raw, device):\n        net.eval()\n        print(\"Changed to eval\")\n        net.to(device)\n        calc = ObjectDetectionMetricsCalculator(80, 0.1)\n        for i, (X, YRaw) in enumerate(test_iter_raw):\n            if i % 1000 = 0:\n                print(f'Calculating {i+1}...')\n            #to_tensor = torchvision.transforms.ToTensor()\n            #X = to_tensor(img).unsqueeze_(0).to(device)\n            X = X.to(device)\n            YHat = net(X)\n            for yhat, yraw in zip(YHat, YRaw):\n                yhat = nms(yhat)\n                calc.add_image_data(yhat.cpu(), yraw)\n        print(\"Finished calculating\")\n        print(\"mAP on validation:\", calculate_mAP(0.5, InterpolationMethod.Interpolation_11))\n        #for i in range(80):\n            #draw_precision_recall(calc.calculate_precision_recall(0.5, i), i)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T21:52:43.913915Z","iopub.execute_input":"2024-04-27T21:52:43.914293Z","iopub.status.idle":"2024-04-27T21:52:43.921441Z","shell.execute_reply.started":"2024-04-27T21:52:43.914264Z","shell.execute_reply":"2024-04-27T21:52:43.920343Z"},"trusted":true},"execution_count":208,"outputs":[]},{"cell_type":"code","source":"from IPython import display\n\nraw = RawDataforTest(val_data)\niter_raw = get_loader(raw)\ntest_and_draw_mAP(net, iter_raw, torch.device('cuda'))","metadata":{"execution":{"iopub.status.busy":"2024-04-27T22:42:03.061215Z","iopub.execute_input":"2024-04-27T22:42:03.061605Z","iopub.status.idle":"2024-04-27T22:42:03.067495Z","shell.execute_reply.started":"2024-04-27T22:42:03.061578Z","shell.execute_reply":"2024-04-27T22:42:03.066538Z"},"trusted":true},"execution_count":240,"outputs":[{"name":"stdout","text":"Changed to eval\nCalculating 1...\nCalculating 2...\nCalculating 3...\nCalculating 4...\nFinished calculating\nmAP on validation: 0.16416070\n","output_type":"stream"}]}]}